# prepare to use Qudrant 


import os
import sys
import openai
from openai import OpenAI


from dotenv import load_dotenv
from flask import Flask, request, abort
from langchain_openai import ChatOpenAI

from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory

from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, AudioMessage, TextMessage, TextSendMessage

# ä½¿ç”¨pdf.py ä¸­çš„get_qa_chainå‡½æ•¸
# from src.pdf import create_vector, load_vectorstore
from src.pdf import load_vectorstore

# from src.pdf import create_vector

# load environment variable
load_dotenv()


# get channel_access_token from my environment variable
channel_access_token = os.getenv("LINE_ACCESS_TOKEN")
channel_secret = os.getenv("LINE_SECRET")

if channel_secret is None:
    print("Specify LINE_CHANNEL_SECRET as environment variable.")
    sys.exit(1)
if channel_access_token is None:
    print("Specify LINE_CHANNEL_ACCESS_TOKEN as environment variable.")
    sys.exit(1)


# create a Flask app

app = Flask(__name__)
line_bot_api = LineBotApi(channel_access_token)
handler = WebhookHandler(channel_secret)
parser = WebhookHandler(channel_secret)
# create a route for webhook

openai_api_key = os.getenv("OPENAI_API_KEY")

model = ChatOpenAI(
    model="gpt-3.5-turbo",
    openai_api_key=openai_api_key,
    temperature=0,
    # streamling=True,
    # streamling means that the model response will be generated in a streaming fashion
)
# vectorstore = create_vector()

# ç¢ºä¿ FAISS å‘é‡å­˜å„²ç›®éŒ„å­˜åœ¨
vectorstore_path = os.getenv("FAISS_VECTORSTORE_PATH")

if vectorstore_path:
    os.makedirs(vectorstore_path, exist_ok=True)
else:
    print("FAISS_VECTORSTORE_PATH is not set.")

vectorstore = load_vectorstore()

embeddings = OpenAIEmbeddings()
# vectorstore = FAISS.load_local("faiss_midjourney_docs", embeddings)
# vectorstore.save_local("faiss_midjourney_docs")

# Load the FAISS Vector Store with Dangerous Deserialization Enabled
try:
    retriever = FAISS.load_local(
        "faiss_vectorstore", embeddings, allow_dangerous_deserialization=True
    ).as_retriever(search_type="similarity", search_kwargs={"k": 1})
except Exception as e:
    print(f"Error loading vectorstore: {e}")


memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    inupt_key="question",
    output_key="answer",
)
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=model,
    memory=memory,
    retriever=retriever,
    verbose=True,
)


@app.route("/callback", methods=["POST"])
def callback():
    # ç²å– LINE çš„ç°½å
    signature = request.headers["X-Line-Signature"]
    body = request.get_data(as_text=True)

    app.logger.info("Request body: " + body)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return "OK"


@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    question = event.message.text.strip()

    if question.startswith("/æ¸…é™¤") or question.lower().startswith("/clear"):
        memory.clear()
        answer = "æ­·å²è¨Šæ¯æ¸…é™¤æˆåŠŸ"
    elif (
        question.startswith("/æ•™å­¸")
        or question.startswith("/æŒ‡ä»¤")
        or question.startswith("/èªªæ˜")
        or question.startswith("/æ“ä½œèªªæ˜")
        or question.lower().startswith("/instruction")
        or question.lower().startswith("/help")
    ):
        answer = "æŒ‡ä»¤ï¼š\n/æ¸…é™¤ or /clear\nğŸ‘‰ ç•¶ Bot é–‹å§‹é¬¼æ‰“ç‰†ï¼Œå¯æ¸…é™¤æ­·å²è¨Šæ¯ä¾†é‡ç½®"
    else:

        # logger.info(f"question language: {question_lang}")

        # get answer from qa_chain
        response = qa_chain({"question": question})
        answer = response["answer"]

    # reply message
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=answer))


@handler.add(MessageEvent, message=AudioMessage)
# è™•ç†æ¥æ”¶åˆ°çš„éŸ³è¨Šè¨Šæ¯
def handle_AudioMessage(event):

    if event.message.type == "audio":
        audio_content = line_bot_api.get_message_content(event.message.id)
        path = "./temp.m4a"

        with open(path, "wb") as fd:
            for chunk in audio_content.iter_content():
                fd.write(chunk)
        # è¼‰å…¥éŸ³æª” å‘¼å«openai api æ¨¡å‹ ä¸¦é€²è¡ŒèªéŸ³è½‰æ›æ–‡å­—

        if os.path.exists(path):

            try:
                # audio_file = open("/Users/your_username/Documents/temp
                # .mp3", "rb")
                with open("./temp.m4a", "rb") as audio_file:
                    openai_api_key = os.getenv("OPENAI_API_KEY")
                    model_id = "whisper-1"
                    client = OpenAI()
                    response = client.audio.transcriptions.create(
                        model=model_id, file=audio_file
                    )

                    # get answer from qa_chain
                    if response and hasattr(response, "text"):
                        # print("here is the response:", response.text)

                        # æ ¹æ“šè½‰æ›çš„æ–‡å­—é€²è¡Œ Q&A è™•ç†
                        response_qa = qa_chain({"question": response.text})
                        answer = response_qa["answer"]

                        # å›æ‡‰ç”¨æˆ¶
                        line_bot_api.reply_message(
                            event.reply_token, TextSendMessage(text=answer)
                        )
                    else:
                        # å¦‚æœ response ä¸åŒ…å« textï¼Œçµ¦å‡ºéŒ¯èª¤æç¤º
                        line_bot_api.reply_message(
                            event.reply_token,
                            TextSendMessage(text="ç„¡æ³•å–å¾—èªéŸ³è½‰æ–‡å­—çš„çµæœ"),
                        )

            except openai.BadRequestError as e:
                # except openai.error.OpenAIError as e:
                print(f"OpenAI API request failed: {e}")
                line_bot_api.reply_message(
                    event.reply_token,
                    TextSendMessage(text="èªéŸ³è½‰æ›æ–‡å­—å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"),
                )


if __name__ == "__main__":
    port = int(os.getenv("PORT", 5000))
    app.run(host="0.0.0.0", port=port, debug=True)
    print("The server has started successfully")
